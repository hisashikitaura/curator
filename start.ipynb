{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nemo_curator.download import download_wikipedia\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "print(cur_dir)\n",
    "# data_dir = f\"{cur_dir}/\"\n",
    "data_dir = \"/workspace/\"\n",
    "\n",
    "cluster = LocalCluster(n_workers=48, processes=True, memory_limit='38GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "# Output\n",
    "download_base_directory= os.path.join(data_dir, \"wiki_downloads\")\n",
    "download_output_directory = os.path.join(download_base_directory, \"data\")\n",
    "print(download_output_directory)\n",
    "\n",
    "# Relevant parameters\n",
    "dump_date = \"20241001\"\n",
    "language = 'ja'\n",
    "url_limit = 1  # 1 file　(jawiki-20240801-pages-articles-multistream1.xml-p1p114794.bz2)\n",
    "\n",
    "res = download_wikipedia(\n",
    "    download_output_directory,\n",
    "    language=language, \n",
    "    dump_date=dump_date,\n",
    "    url_limit=url_limit\n",
    ").df.compute()\n",
    "\n",
    "#client.cluster.close()\n",
    "#client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 13:26:19,905 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,910 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,915 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,921 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,926 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,935 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,942 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,949 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,956 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,961 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,971 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,977 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,984 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,990 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:19,996 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,002 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,009 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,015 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,022 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,028 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,034 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,041 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,048 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,058 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,063 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,069 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,076 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,081 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,087 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,094 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,100 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,105 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,115 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,129 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,138 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,151 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,156 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,162 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,168 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,174 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,180 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,185 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,192 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,200 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,211 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,217 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,224 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n",
      "2024-11-23 13:26:20,231 - distributed.nanny.memory - WARNING - Ignoring provided memory limit 38GB due to system memory limit of 31.29 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-23 13:26:32--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.166.244.93, 3.166.244.8, 3.166.244.49, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.166.244.93|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 131266198 (125M) [application/octet-stream]\n",
      "Saving to: ‘/workspace/language_sep/lid.176.bin.4’\n",
      "\n",
      "lid.176.bin.4       100%[===================>] 125.18M  11.5MB/s    in 12s     \n",
      "\n",
      "2024-11-23 13:26:44 (10.7 MB/s) - ‘/workspace/language_sep/lid.176.bin.4’ saved [131266198/131266198]\n",
      "\n",
      "Reading 1 files\n",
      "read_kwargs={'lines': True, 'dtype': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df[metadata_field]=0        JA\n",
      "1        JA\n",
      "2        JA\n",
      "3        JA\n",
      "4        JA\n",
      "         ..\n",
      "59649    JA\n",
      "59650    JA\n",
      "59651    JA\n",
      "59652    JA\n",
      "59653    JA\n",
      "Name: language, Length: 59628, dtype: object\n",
      "Time taken for splitting language:126.00551414489746\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "from nemo_curator import ScoreFilter,Modify\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.filters import FastTextLangId\n",
    "from nemo_curator.modifiers import UnicodeReformatter\n",
    "from nemo_curator.utils.file_utils import separate_by_metadata\n",
    "\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "print(cur_dir)\n",
    "# data_dir = f\"{cur_dir}/\"\n",
    "data_dir = \"/workspace/\"\n",
    "\n",
    "# 前の処理でclusterを落としている場合は以下をアンコメントして再度起動してください\n",
    "cluster = LocalCluster(n_workers=48, processes=True, memory_limit='38GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "# Input path\n",
    "# multilingual_data_path = \"./wiki_downloads/data/jawiki-20240801-pages-articles-multistream1.xml-p1p114794.bz2.jsonl\"\n",
    "multilingual_data_path = \"wiki_downloads/data/jawiki-20241001-pages-articles-multistream1.xml-p1p114794.bz2.jsonl\"\n",
    "# multilingual_data_path = \"wiki_downloads/data/jawiki-20241001-pages-articles-multistream1.added_meta.xml-p1p114794.bz2.jsonl\"\n",
    "multilingual_data_full_path = os.path.join(data_dir, multilingual_data_path)\n",
    "\n",
    "# Output path\n",
    "language_base_output_path = os.path.join(data_dir,\"language_sep\")\n",
    "language_data_output_path = os.path.join(language_base_output_path,\"data\")\n",
    "language_separated_output_path = os.path.join(language_data_output_path,\"language\")\n",
    "\n",
    "# Fasttext model path\n",
    "model_path = language_base_output_path\n",
    "\n",
    "# Define key in output .jsonl files to store the language information\n",
    "language_field = \"language\"\n",
    "# print(f\"{model_path=}\")\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -P {model_path}\n",
    "t0 = time.time()\n",
    "\n",
    "# Load dataset \n",
    "# multilingual_dataset = DocumentDataset.read_json(multilingual_data_full_path, add_filename=True, dtype={\"text\": str, \"title\": str, \"id\": \"int64\", \"url\":str, \"language\":str, \"source_id\": str, \"filename\": str})\n",
    "multilingual_dataset = DocumentDataset.read_json(multilingual_data_full_path, add_filename=True, input_meta={**{\"text\": str}, **{\"title\": str}, **{\"id\": \"int64\"}, **{\"url\":str}, **{\"language\":str}, **{\"source_id\": str}, **{\"filename\": str}})\n",
    "# multilingual_dataset = DocumentDataset.read_json(multilingual_data_full_path, add_filename=True, input_meta={\"id\": int})\n",
    "# # multilingual_dataset = DocumentDataset.read_json(multilingual_data_full_path, add_filename=True)\n",
    "\n",
    "# Define Language separation pipeline\n",
    "lang_filter = FastTextLangId(os.path.join(model_path,'lid.176.bin'))\n",
    "language_id_pipeline = ScoreFilter(lang_filter, score_field=language_field, score_type='object')\n",
    "filtered_dataset = language_id_pipeline(multilingual_dataset)\n",
    "# print(f\"{filtered_dataset=}\")\n",
    "# print(f\"{filtered_dataset.df=}\")\n",
    "# print(f\"{filtered_dataset.df[language_field]=}\")\n",
    "\n",
    "# The language separation pipeline will produce a result looks like ['JA',0.96873], we only want to keep the 'JA' label and drop the detailed classifier score\n",
    "filtered_dataset.df[language_field] = filtered_dataset.df[language_field].apply(lambda score: score[1],meta = (language_field, 'object'))\n",
    "# print(f\"{filtered_dataset.df[language_field]=}\")\n",
    "# Split the dataset to corresponding language sub-folders\n",
    "language_stats = separate_by_metadata(filtered_dataset.df, language_separated_output_path, metadata_field=language_field).compute()\n",
    "\n",
    "print(f\"Time taken for splitting language:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n",
      "read_kwargs={'lines': True, 'dtype': False}\n",
      "read_kwargs={'lines': True, 'dtype': False}\n",
      "Writing to disk complete for 1 partitions\n",
      "Time taken for fixing unicode:461.9042181968689\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Define desired language\n",
    "target_language = \"JA\"\n",
    "\n",
    "# Output path\n",
    "lang_sep_cleaned_data_output_path = os.path.join(language_data_output_path, \"cleaned\")\n",
    "\n",
    "# Read the language specific data and fix the unicode in it\n",
    "lang_data_path = os.path.join(language_separated_output_path, target_language)\n",
    "lang_data = DocumentDataset.read_json(lang_data_path,add_filename=True)\n",
    "\n",
    "cleaner = Modify(UnicodeReformatter())\n",
    "cleaned_data = cleaner(lang_data)\n",
    "\n",
    "# Write the cleaned_data\n",
    "cleaned_data.to_json(lang_sep_cleaned_data_output_path, write_to_filename=True)\n",
    "\n",
    "print(f\"Time taken for fixing unicode:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n",
      "read_kwargs={'lines': True, 'dtype': False}\n",
      "Writing to disk complete for 1 partitions\n",
      "Time taken for add ID:48.32787084579468\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from nemo_curator import AddId\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "# data_dir = f\"{cur_dir}/\"\n",
    "data_dir = \"/workspace/\"\n",
    "\n",
    "# Input\n",
    "add_id_input_data_dir = os.path.join(data_dir, \"language_sep/data/cleaned\")\n",
    "\n",
    "# Output\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "\n",
    "# Format of output ID will be <prefix>_<id>, Define prefix here\n",
    "add_ID_id_prefix=\"JA_wiki\"\n",
    "\n",
    "t0 = time.time()\n",
    "# Read input files\n",
    "dataset = DocumentDataset.read_json(add_id_input_data_dir,add_filename=True)\n",
    "\n",
    "# Run AddID() on the input dataset\n",
    "add_id = AddId(id_field='id',id_prefix=add_ID_id_prefix,start_index=0)\n",
    "id_dataset = add_id(dataset)\n",
    "\n",
    "# Output files\n",
    "id_dataset.to_json(added_id_output_path, write_to_filename=True)\n",
    "\n",
    "print(f\"Time taken for add ID:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DASK_DATAFRAME__QUERY_PLANNING=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 35027 instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/dask_cuda/utils.py:170: UserWarning: Cannot get CPU affinity for device with index 0, setting default affinity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dask worker:1\n",
      "Reading 1 files\n",
      "Number of exact duplicated file:2\n",
      "Time taken for exact duplicate:15.952196598052979\n",
      "Number of exact duplicated document:2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_hashes</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0d3ffcf18f61d821db277db357fa7b0e</td>\n",
       "      <td>JA_wiki-0000052756 JA_wiki-0000052758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _hashes                                     id\n",
       "0  0d3ffcf18f61d821db277db357fa7b0e  JA_wiki-0000052756 JA_wiki-0000052758"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env DASK_DATAFRAME__QUERY_PLANNING=False\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.modules import ExactDuplicates\n",
    "from nemo_curator.utils.distributed_utils import get_client, get_num_workers\n",
    "\n",
    "def pre_imports():\n",
    "    import cudf \n",
    "\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "# data_dir = f\"{cur_dir}/\"\n",
    "data_dir = \"/workspace/\"\n",
    "\n",
    "\n",
    "client = get_client(cluster_type='gpu', set_torch_to_use_rmm=False)\n",
    "print(f\"Number of dask worker:{get_num_workers(client)}\")\n",
    "client.run(pre_imports)\n",
    "\n",
    "# Input\n",
    "exact_dedup_input_dataset_dir = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "\n",
    "# Output\n",
    "exact_dedup_base_output_path = os.path.join(data_dir, \"exact_dedup\")\n",
    "exact_dedup_log_dir = os.path.join(exact_dedup_base_output_path, 'log')\n",
    "exact_dedup_output_dir = os.path.join(exact_dedup_base_output_path, 'data')\n",
    "\n",
    "# Parameters for ExactDuplicates()\n",
    "exact_dedup_dataset_id_field = \"id\"\n",
    "exact_dedup_dataset_text_field = \"text\"\n",
    "\n",
    "!mkdir -p {exact_dedup_log_dir}\n",
    "!mkdir -p {exact_dedup_output_dir}\n",
    "\n",
    "t0 = time.time()\n",
    "# Read input dataset\n",
    "input_dataset = DocumentDataset.read_json(exact_dedup_input_dataset_dir, backend='cudf')\n",
    "\n",
    "# Run exact deduplication to the input\n",
    "exact_dup = ExactDuplicates(\n",
    "    logger=exact_dedup_log_dir,\n",
    "    id_field=exact_dedup_dataset_id_field,\n",
    "    text_field=exact_dedup_dataset_text_field,\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=exact_dedup_output_dir  # Duplicated document ID list is output to the cache_dir\n",
    ")\n",
    "duplicates = exact_dup(dataset=input_dataset)\n",
    "\n",
    "print(f\"Number of exact duplicated file:{len(duplicates)}\")\n",
    "print(f\"Time taken for exact duplicate:{time.time()-t0}\")\n",
    "\n",
    "exact_dedup_res = pd.read_parquet(os.path.join(exact_dedup_output_dir, \"_exact_duplicates.parquet\"))\n",
    "print(f\"Number of exact duplicated document:{len(exact_dedup_res)}\")\n",
    "exact_dedup_res.head()\n",
    "\n",
    "exact_dedup_res.groupby('_hashes')['id'].agg(lambda x: ' '.join(x)).reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DASK_DATAFRAME__QUERY_PLANNING=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 41351 instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/dask_cuda/utils.py:170: UserWarning: Cannot get CPU affinity for device with index 0, setting default affinity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dask worker:1\n",
      "Reading 1 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/NeMo-Curator/nemo_curator/modules/config.py:91: UserWarning: Identifying false positives during the Minhash deduplication is computationally expensive. For improved performance consider setting this to False\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage1: Starting Minhash + LSH computation\n",
      "Stage1: Minhash + LSH complete!\n",
      "Stage2 (False Positive Check): Starting Map_Buckets\n",
      "Stage2 (False Postive Check): Map_Buckets Complete!\n",
      "Stage3 (False Postive Check): Shuffle docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started processing bucket-map partitions 0 through 1 of 1\n",
      "Using 1 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 210 rows to disk\n",
      "Text-df partition  1/1 completed in 7.459746599197388\n",
      "Bucket partition  1/1 completed in 7.475650310516357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage3 (False Postive Check): Shuffle docs complete!\n",
      "Stage4 (False Postive Check): Jaccard Similarity in Buckets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage4 (False Postive Check): Jaccard Similarity in Buckets Complete!\n",
      "Stage5: Connected Components across buckets\n",
      "batch_id = 0/1, time = 0.4316842555999756\n",
      "[1732369704.916822] [docker-desktop:19625:0]          parser.c:2305 UCX  WARN  unused environment variable: UCX_MEMTYPE_CACHE (maybe: UCX_MEMTYPE_CACHE?)\n",
      "[1732369704.916822] [docker-desktop:19625:0]          parser.c:2305 UCX  WARN  (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/multi.py:1324: UserWarning: Concatenating dataframes with unknown divisions.\n",
      "We're assuming that the indices of each dataframes are \n",
      " aligned. This assumption is not generally safe.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/cudf/core/reshape.py:350: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of groups 95\n",
      "# of docs removed 15\n",
      "assert num_nodes:110==labels_df:110 passed\n",
      "Stage5: Connected Components across buckets complete!\n",
      "Writing to disk complete for 1 partitions\n",
      "Time taken for Connected Component: 57.66777467727661 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JA_wiki-0000025154</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JA_wiki-0000045605</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JA_wiki-0000026301</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JA_wiki-0000026251</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JA_wiki-0000025368</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  group\n",
       "0  JA_wiki-0000025154     24\n",
       "1  JA_wiki-0000045605     25\n",
       "2  JA_wiki-0000026301     26\n",
       "3  JA_wiki-0000026251     27\n",
       "4  JA_wiki-0000025368     28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env DASK_DATAFRAME__QUERY_PLANNING=False\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from nemo_curator import FuzzyDuplicates, FuzzyDuplicatesConfig\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.utils.distributed_utils import get_client, get_num_workers\n",
    "\n",
    "import dask\n",
    "\n",
    "def pre_imports():\n",
    "    import cudf \n",
    "\n",
    "    \n",
    "cur_dir = os.getcwd()\n",
    "# data_dir = f\"{cur_dir}/\"\n",
    "data_dir = \"/workspace/\"\n",
    "\n",
    "# client = get_client(cluster_type='gpu', set_torch_to_use_rmm=False)\n",
    "print(f\"Number of dask worker:{get_num_workers(client)}\")\n",
    "# client.run(pre_imports)\n",
    "\n",
    "# Input\n",
    "fuzzy_dedup_data_path = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "\n",
    "# Output\n",
    "fuzzy_dedup_base_output_path = os.path.join(data_dir, \"fuzzy_wrapper\")\n",
    "fuzzy_dedup_log_dir = os.path.join(fuzzy_dedup_base_output_path, 'log')\n",
    "fuzzy_dedup_cache_dir = os.path.join(fuzzy_dedup_base_output_path, 'cache')\n",
    "fuzzy_dedup_output_dir = os.path.join(fuzzy_dedup_base_output_path, 'data')\n",
    "\n",
    "# Relevant parameters\n",
    "id_field = 'id'\n",
    "text_field = 'text'\n",
    "filetype = \"parquet\"\n",
    "\n",
    "!mkdir -p {fuzzy_dedup_base_output_path}\n",
    "!mkdir -p {fuzzy_dedup_log_dir}\n",
    "!mkdir -p {fuzzy_dedup_cache_dir}\n",
    "!mkdir -p {fuzzy_dedup_output_dir}\n",
    "\n",
    "#!rm -r {fuzzy_dedup_cache_dir}\n",
    "\n",
    "with dask.config.set({\"dataframe.backend\": 'cudf'}):\n",
    "        \n",
    "    t0 = time.time()\n",
    "        \n",
    "    input_dataset = DocumentDataset.read_json(fuzzy_dedup_data_path, backend='cudf')\n",
    "    fuzzy_dedup_config = FuzzyDuplicatesConfig(\n",
    "        cache_dir=fuzzy_dedup_cache_dir,\n",
    "        id_field=id_field,\n",
    "        text_field=text_field,\n",
    "        seed=42,  # Use the seed set in Minhash section for consistency\n",
    "        char_ngrams=5,\n",
    "        num_buckets=20,\n",
    "        hashes_per_bucket=13,\n",
    "        use_64_bit_hash=False,\n",
    "        buckets_per_shuffle=5,\n",
    "        false_positive_check=True,\n",
    "        num_anchors=2,\n",
    "        jaccard_threshold=0.8,\n",
    "    )\n",
    "    fuzzy_dup = FuzzyDuplicates(logger=fuzzy_dedup_log_dir, config=fuzzy_dedup_config)\n",
    "    duplicates = fuzzy_dup(dataset=input_dataset)\n",
    "        \n",
    "    duplicates.to_parquet(fuzzy_dedup_output_dir, write_to_filename=False)\n",
    "       \n",
    "    print(f\"Time taken for Connected Component: {time.time()-t0} s\")    \n",
    "        \n",
    "fuzzy_dedup_res = pd.read_parquet(fuzzy_dedup_output_dir)\n",
    "fuzzy_dedup_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n",
      "Reading 1 files\n",
      "Length of duplicate removed dataset:59508\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "# data_dir = f\"{cur_dir}/\"\n",
    "data_dir = \"/workspace/\"\n",
    "\n",
    "# Input\n",
    "dataset_dir = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "exact_dedup_output_dir= os.path.join(data_dir, \"exact_dedup/data\")\n",
    "fuzzy_dedup_output_dir= os.path.join(data_dir, \"fuzzy_wrapper/data\")\n",
    "\n",
    "# Output\n",
    "dudped_output_dir = os.path.join(data_dir, \"remove_duplicate/result.parquet\")\n",
    "\n",
    "# Relevant parameters\n",
    "input_id_field = 'id'\n",
    "id_prefix = \"JA_wiki\"\n",
    "\n",
    "!mkdir -p {dudped_output_dir}\n",
    "\n",
    "#Load .jsonl dataset (GPUメモリが足りない場合はbackend='pandas'へ変更してください)\n",
    "input_dataset = DocumentDataset.read_json(dataset_dir, backend='cudf')\n",
    "\n",
    "# Load exact deduplicate result and extract list of duplicated document ID　(GPUメモリが足りない場合はbackend='pandas'へ変更してください)\n",
    "exact_duplicates = DocumentDataset.read_parquet(os.path.join(exact_dedup_output_dir, \"_exact_duplicates.parquet\"), backend='cudf')\n",
    "exact_docs_to_remove = exact_duplicates.df.map_partitions(\n",
    "    lambda x: x[x._hashes.duplicated(keep=\"first\")]\n",
    ")\n",
    "\n",
    "# Remove the duplicated document from input dataset\n",
    "result = input_dataset.df[\n",
    "    ~input_dataset.df[input_id_field].isin(exact_docs_to_remove[input_id_field].compute())\n",
    "]\n",
    "\n",
    "# Loads result from fuzzy dedup wrapper\n",
    "fuzzy_duplicates = pd.read_parquet(fuzzy_dedup_output_dir)\n",
    "\n",
    "# Generate list of near duplicate document ID\n",
    "fuzzy_docs_to_remove = fuzzy_duplicates.drop_duplicates(subset=['group'], keep='first')\n",
    "\n",
    "# Remove near duplicates\n",
    "result = result[~result[input_id_field].isin(fuzzy_docs_to_remove[input_id_field])]\n",
    "\n",
    "# Save final result to local (backend='pandas'の場合は、write_to_filename=Trueをコメントアウトしてください)\n",
    "result.to_parquet(dudped_output_dir, write_to_filename=True)\n",
    "\n",
    "res = pd.read_parquet(dudped_output_dir)\n",
    "print(f\"Length of duplicate removed dataset:{len(res)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "data_dir = f\"{cur_dir}/\"\n",
    "dotenv_path = os.path.join(data_dir, \".env\")\n",
    "load_dotenv(dotenv_path, verbose=True)\n",
    "nvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=nvidia_api_key  # 取得したAPIキーを入力してください\n",
    ")\n",
    "\n",
    "n_subtopics = 2\n",
    "n_questions = 2\n",
    "topic = \"機械学習\"\n",
    "\n",
    "TOPIC_GENERATION_PROMPT_TEMPLATE = \"\"\"\\\n",
    "トピックが与えられた場合、そのトピックに関連する {n_subtopics} のサブトピックのリストを生成してください。\n",
    "トピックは：{topic}\n",
    "リストは番号なしで、サブトピックの説明なしでなければなりません。サブトピックはコンマで区切られる必要があります。リスト以外のテキストは存在してはなりません。\n",
    "\"\"\"\n",
    "\n",
    "QUESTION_PROMPT_TEMPLATE = \"\"\"\\\n",
    "トピックが与えられた場合、そのトピックに関して{n_questions}個の質問を生成してください。\n",
    "トピックは：{sub_topic}\n",
    "リスト形式で、質問は改行文字で区切られる必要があります。リスト以外のテキストは存在してはなりません。\n",
    "\"\"\"\n",
    "\n",
    "RESPONSE_PROMPT_TEMPLATE = \"\"\"\\\n",
    "質問が与えられた場合、その質問に対して考えられる2つの回答を生成してください。\n",
    "質問は：{question}\n",
    "リスト形式は以下の形式である必要があります：\n",
    "\n",
    "RESPONSE A: ここに回答Aのテキストを入力\n",
    "RESPONSE B: ここに回答Bのテキストを入力\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ディープラーニング、強化学習']\n"
     ]
    }
   ],
   "source": [
    "# data_dir = \"/workspace/\"\n",
    "\n",
    "# print(os.environ[\"NVIDIA_API_KEY\"])\n",
    "# print(os.environ.get(\"NVIDIA_API_KEY\"))\n",
    "\n",
    "# generate sub topics\n",
    "async def generate_subtopics(client, topic, n_subtopics):\n",
    "    prompt = TOPIC_GENERATION_PROMPT_TEMPLATE.format(topic=topic, n_subtopics=n_subtopics)\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"meta/llama-3.1-405b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\" : \"user\",\n",
    "             \"content\" : prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "subtopics = await generate_subtopics(client, topic, n_subtopics)\n",
    "subtopic_list = subtopics.choices[0].message.content.split(\",\")\n",
    "print(subtopic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ディープラーニングと強化学習の違いは何ですか？\\n\\nディープラーニングと強化学習を組み合わせた研究事例を教えてください。']\n"
     ]
    }
   ],
   "source": [
    "# generate questions of sub topics\n",
    "async def generate_questions(client, sub_topic, n_questions):\n",
    "    prompt = QUESTION_PROMPT_TEMPLATE.format(sub_topic=sub_topic, n_questions=n_questions)\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"meta/llama-3.1-405b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\" : \"user\",\n",
    "             \"content\" : prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    if hasattr(response, 'choices') and response.choices:\n",
    "        return response.choices[0].message.content\n",
    "    else:\n",
    "        print(f\"Unexpected response structure: {response}\")\n",
    "        return None\n",
    "\n",
    "async def question_generator(client, subtopic_list, n_question):\n",
    "    tasks = [generate_questions(client, subtopic, n_question) for subtopic in subtopic_list]\n",
    "    question_list = await asyncio.gather(*tasks)\n",
    "    return question_list\n",
    "\n",
    "nest_asyncio.apply()\n",
    "question_list = asyncio.run(question_generator(client, subtopic_list, n_questions))\n",
    "print(question_list)\n",
    "\n",
    "# format questions\n",
    "question_list_formatted = []\n",
    "for question_set in question_list:\n",
    "    question_list_formatted += question_set.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RESPONSE A: ディープラーニングは、主に大規模なデータセットを使用してパターンを学習し、画像や音声などのデータを認識することに重点を置いた機械学習の一種です。一方、強化学習は、エージェントが環境とやり取りし、報酬を最大化する行動を学習するプロセスに重点を置いています。\\n\\nRESPONSE B: ディープラーニングは、ニューラルネットワークを使用してデータの複雑なパターンを学習する手法であり、画像認識や自然言語処理などの分野で広く使用されています。強化学習は、エージェントが環境から得られる報酬に基づいて行動を学習し、ロボット工学やゲームなどの分野で応用されています。', 'RESPONSE A: AlphaGoはディープラーニングと強化学習を組み合わせた研究事例として有名です。AlphaGoは、Google DeepMindによって開発されたコンピュータープログラムで、囲碁のゲームをプレイするために設計されました。AlphaGoは、ディープラーニングを使用してゲームの状態を評価し、強化学習を使用してゲームの戦略を学習しました。AlphaGoは、2016年に世界のトッププレイヤーである李世乭を破り、コンピュータープログラムが人間の世界チャンピオンを破った初めての例となりました。\\n\\nRESPONSE B: 自動運転車の開発におけるディープラーニングと強化学習の組み合わせも注目されています。自動運転車は、ディープラーニングを使用して道路の状況を認識し、強化学習を使用して運転の戦略を学習します。強化学習アルゴリズムは、自動運転車がさまざまな状況で安全かつ効率的に運転する方法を学習するために使用されます。例えば、自動運転車は強化学習を使用して、道路の状況に応じて速度を調整したり、障害物を回避したりする方法を学習できます。このような研究事例は、自動運転車の安全性と性能を向上させるために重要です。']\n"
     ]
    }
   ],
   "source": [
    "# generate response of each question\n",
    "async def generate_responses(client, question):\n",
    "    prompt = RESPONSE_PROMPT_TEMPLATE.format(question=question)\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"meta/llama-3.1-405b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\" : \"user\",\n",
    "             \"content\" : prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    if hasattr(response, 'choices') and response.choices:\n",
    "        return response.choices[0].message.content\n",
    "    else:\n",
    "        print(f\"Unexpected response structure: {response}\")\n",
    "        return None\n",
    "\n",
    "async def response_generator(client, question_list):\n",
    "    tasks = [generate_responses(client, question) for question in question_list]\n",
    "    response_list = await asyncio.gather(*tasks)\n",
    "    return response_list\n",
    "\n",
    "question_response_list = asyncio.run(response_generator(client, question_list_formatted))\n",
    "print(question_response_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item={'question': 'ディープラーニングと強化学習の違いは何ですか？', 'responses': {'response_a': {'response': 'ディープラーニングは、主に大規模なデータセットを使用してパターンを学習し、画像や音声などのデータを認識することに重点を置いた機械学習の一種です。一方、強化学習は、エージェントが環境とやり取りし、報酬を最大化する行動を学習するプロセスに重点を置いています。'}, 'response_b': {'response': 'ディープラーニングは、ニューラルネットワークを使用してデータの複雑なパターンを学習する手法であり、画像認識や自然言語処理などの分野で広く使用されています。強化学習は、エージェントが環境から得られる報酬に基づいて行動を学習し、ロボット工学やゲームなどの分野で応用されています。'}}}\n",
      "item={'question': 'ディープラーニングと強化学習を組み合わせた研究事例を教えてください。', 'responses': {'response_a': {'response': 'AlphaGoはディープラーニングと強化学習を組み合わせた研究事例として有名です。AlphaGoは、Google DeepMindによって開発されたコンピュータープログラムで、囲碁のゲームをプレイするために設計されました。AlphaGoは、ディープラーニングを使用してゲームの状態を評価し、強化学習を使用してゲームの戦略を学習しました。AlphaGoは、2016年に世界のトッププレイヤーである李世乭を破り、コンピュータープログラムが人間の世界チャンピオンを破った初めての例となりました。'}, 'response_b': {'response': '自動運転車の開発におけるディープラーニングと強化学習の組み合わせも注目されています。自動運転車は、ディープラーニングを使用して道路の状況を認識し、強化学習を使用して運転の戦略を学習します。強化学習アルゴリズムは、自動運転車がさまざまな状況で安全かつ効率的に運転する方法を学習するために使用されます。例えば、自動運転車は強化学習を使用して、道路の状況に応じて速度を調整したり、障害物を回避したりする方法を学習できます。このような研究事例は、自動運転車の安全性と性能を向上させるために重要です。'}}}\n"
     ]
    }
   ],
   "source": [
    "# prepare question:response pair set list\n",
    "question_response_pair_list = []\n",
    "for question, response_set in zip(question_list_formatted, question_response_list):\n",
    "    question_response_pair_list.append(\n",
    "        {\n",
    "            \"question\" : question, \n",
    "            \"responses\" : {\n",
    "                \"response_a\" : {\"response\" : response_set.split(\"RESPONSE B:\")[0].replace(\"RESPONSE A:\", \"\").strip().split(\"\\n\\n\")[-1].strip()},\n",
    "                \"response_b\" : {\"response\" : response_set.split(\"RESPONSE B:\")[-1].split(\"\\n\\n\")[0].strip()}\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "import json\n",
    "\n",
    "# export to jsonl file\n",
    "with open('synthetic_data.jsonl', 'w') as f:\n",
    "    for item in question_response_pair_list:\n",
    "        print(f\"{item=}\")\n",
    "        f.write(json.dumps(item, ensure_ascii=False))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'ディープラーニングと強化学習の違いは何ですか？', 'responses': {'response_a': {'response': 'ディープラーニングは、主に大規模なデータセットを使用してパターンを学習し、画像や音声などのデータを認識することに重点を置いた機械学習の一種です。一方、強化学習は、エージェントが環境とやり取りし、報酬を最大化する行動を学習するプロセスに重点を置いています。', 'helpfulness': 3.140625, 'correctness': 3.140625, 'coherence': 3.671875, 'complexity': 1.734375, 'verbosity': 1.3984375}, 'response_b': {'response': 'ディープラーニングは、ニューラルネットワークを使用してデータの複雑なパターンを学習する手法であり、画像認識や自然言語処理などの分野で広く使用されています。強化学習は、エージェントが環境から得られる報酬に基づいて行動を学習し、ロボット工学やゲームなどの分野で応用されています。', 'helpfulness': 2.9375, 'correctness': 2.859375, 'coherence': 3.625, 'complexity': 1.5703125, 'verbosity': 1.3203125}}}, {'question': 'ディープラーニングと強化学習を組み合わせた研究事例を教えてください。', 'responses': {'response_a': {'response': 'AlphaGoはディープラーニングと強化学習を組み合わせた研究事例として有名です。AlphaGoは、Google DeepMindによって開発されたコンピュータープログラムで、囲碁のゲームをプレイするために設計されました。AlphaGoは、ディープラーニングを使用してゲームの状態を評価し、強化学習を使用してゲームの戦略を学習しました。AlphaGoは、2016年に世界のトッププレイヤーである李世乭を破り、コンピュータープログラムが人間の世界チャンピオンを破った初めての例となりました。', 'helpfulness': 3.203125, 'correctness': 3.328125, 'coherence': 3.953125, 'complexity': 1.78125, 'verbosity': 1.5078125}, 'response_b': {'response': '自動運転車の開発におけるディープラーニングと強化学習の組み合わせも注目されています。自動運転車は、ディープラーニングを使用して道路の状況を認識し、強化学習を使用して運転の戦略を学習します。強化学習アルゴリズムは、自動運転車がさまざまな状況で安全かつ効率的に運転する方法を学習するために使用されます。例えば、自動運転車は強化学習を使用して、道路の状況に応じて速度を調整したり、障害物を回避したりする方法を学習できます。このような研究事例は、自動運転車の安全性と性能を向上させるために重要です。', 'helpfulness': 3.03125, 'correctness': 3.15625, 'coherence': 3.703125, 'complexity': 1.75, 'verbosity': 1.671875}}}]\n"
     ]
    }
   ],
   "source": [
    "def get_scores_from_response(openai_response_template):\n",
    "    logprobs = openai_response_template.choices[0].logprobs.content\n",
    "    score_dict = {}\n",
    "    for score in logprobs:\n",
    "        score_dict[score.token] = score.logprob\n",
    "    return score_dict\n",
    "\n",
    "async def get_response_and_scores(client, question, response_content):\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\"content\": question},\n",
    "        {\"role\": \"assistant\",\"content\": response_content},]\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"nvidia/nemotron-4-340b-reward\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    scores = get_scores_from_response(response)\n",
    "    return scores\n",
    "\n",
    "# scoring for question:response pair set\n",
    "async def process_question_response_pairs(client,question_response_score_list):\n",
    "    tasks = []\n",
    "    for question_response_pair in question_response_score_list:\n",
    "        question = question_response_pair[\"question\"]\n",
    "        \n",
    "        task_a = get_response_and_scores(client, question, question_response_pair[\"responses\"][\"response_a\"][\"response\"])\n",
    "        task_b = get_response_and_scores(client, question, question_response_pair[\"responses\"][\"response_b\"][\"response\"])\n",
    "        \n",
    "        tasks.append((task_a, question_response_pair, \"response_a\"))\n",
    "        tasks.append((task_b, question_response_pair, \"response_b\"))\n",
    "    results = await asyncio.gather(*[task[0] for task in tasks])\n",
    "    \n",
    "    for i, (result, task_info) in enumerate(zip(results, tasks)):\n",
    "        _, question_response_pair, response_key = task_info\n",
    "        question_response_pair[\"responses\"][response_key].update(result)\n",
    "question_response_score_list = question_response_pair_list.copy()\n",
    "await process_question_response_pairs(client, question_response_score_list)\n",
    "print(question_response_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare question:response pair set list\n",
    "question_response_pair_list = []\n",
    "for question, response_set in zip(question_list_formatted, question_response_list):\n",
    "    question_response_pair_list.append(\n",
    "        {\n",
    "            \"question\" : question, \n",
    "            \"responses\" : {\n",
    "                \"response_a\" : {\"response\" : response_set.split(\"RESPONSE B:\")[0].replace(\"RESPONSE A:\", \"\").strip().split(\"\\n\\n\")[-1].strip()},\n",
    "                \"response_b\" : {\"response\" : response_set.split(\"RESPONSE B:\")[-1].split(\"\\n\\n\")[0].strip()}\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "import json\n",
    "\n",
    "# export to jsonl file\n",
    "with open('synthetic_data.jsonl', 'w') as f:\n",
    "    for item in question_response_pair_list:\n",
    "        f.write(json.dumps(item, ensure_ascii=False))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'ディープラーニングと強化学習の違いは何ですか？', 'responses': {'response_a': {'response': 'ディープラーニングは、主に大規模なデータセットを使用してパターンを学習し、画像や音声などのデータを認識することに重点を置いた機械学習の一種です。一方、強化学習は、エージェントが環境とやり取りし、報酬を最大化する行動を学習するプロセスに重点を置いています。', 'helpfulness': 3.140625, 'correctness': 3.140625, 'coherence': 3.671875, 'complexity': 1.734375, 'verbosity': 1.3984375}, 'response_b': {'response': 'ディープラーニングは、ニューラルネットワークを使用してデータの複雑なパターンを学習する手法であり、画像認識や自然言語処理などの分野で広く使用されています。強化学習は、エージェントが環境から得られる報酬に基づいて行動を学習し、ロボット工学やゲームなどの分野で応用されています。', 'helpfulness': 2.9375, 'correctness': 2.859375, 'coherence': 3.625, 'complexity': 1.5703125, 'verbosity': 1.3203125}}}, {'question': 'ディープラーニングと強化学習を組み合わせた研究事例を教えてください。', 'responses': {'response_a': {'response': 'AlphaGoはディープラーニングと強化学習を組み合わせた研究事例として有名です。AlphaGoは、Google DeepMindによって開発されたコンピュータープログラムで、囲碁のゲームをプレイするために設計されました。AlphaGoは、ディープラーニングを使用してゲームの状態を評価し、強化学習を使用してゲームの戦略を学習しました。AlphaGoは、2016年に世界のトッププレイヤーである李世乭を破り、コンピュータープログラムが人間の世界チャンピオンを破った初めての例となりました。', 'helpfulness': 3.203125, 'correctness': 3.328125, 'coherence': 3.953125, 'complexity': 1.78125, 'verbosity': 1.5078125}, 'response_b': {'response': '自動運転車の開発におけるディープラーニングと強化学習の組み合わせも注目されています。自動運転車は、ディープラーニングを使用して道路の状況を認識し、強化学習を使用して運転の戦略を学習します。強化学習アルゴリズムは、自動運転車がさまざまな状況で安全かつ効率的に運転する方法を学習するために使用されます。例えば、自動運転車は強化学習を使用して、道路の状況に応じて速度を調整したり、障害物を回避したりする方法を学習できます。このような研究事例は、自動運転車の安全性と性能を向上させるために重要です。', 'helpfulness': 3.03125, 'correctness': 3.15625, 'coherence': 3.703125, 'complexity': 1.75, 'verbosity': 1.671875}}}]\n"
     ]
    }
   ],
   "source": [
    "# running reward scoring model to evaluate the responses \n",
    "def get_scores_from_response(openai_response_template):\n",
    "    logprobs = openai_response_template.choices[0].logprobs.content\n",
    "    score_dict = {}\n",
    "    for score in logprobs:\n",
    "        score_dict[score.token] = score.logprob\n",
    "    return score_dict\n",
    "\n",
    "async def get_response_and_scores(client, question, response_content):\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\"content\": question},\n",
    "        {\"role\": \"assistant\",\"content\": response_content},]\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"nvidia/nemotron-4-340b-reward\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    scores = get_scores_from_response(response)\n",
    "    return scores\n",
    "\n",
    "# scoring for question:response pair set\n",
    "async def process_question_response_pairs(client,question_response_score_list):\n",
    "    tasks = []\n",
    "    for question_response_pair in question_response_score_list:\n",
    "        question = question_response_pair[\"question\"]\n",
    "        \n",
    "        task_a = get_response_and_scores(client, question, question_response_pair[\"responses\"][\"response_a\"][\"response\"])\n",
    "        task_b = get_response_and_scores(client, question, question_response_pair[\"responses\"][\"response_b\"][\"response\"])\n",
    "        \n",
    "        tasks.append((task_a, question_response_pair, \"response_a\"))\n",
    "        tasks.append((task_b, question_response_pair, \"response_b\"))\n",
    "    results = await asyncio.gather(*[task[0] for task in tasks])\n",
    "    \n",
    "    for i, (result, task_info) in enumerate(zip(results, tasks)):\n",
    "        _, question_response_pair, response_key = task_info\n",
    "        question_response_pair[\"responses\"][response_key].update(result)\n",
    "question_response_score_list = question_response_pair_list.copy()\n",
    "await process_question_response_pairs(client, question_response_score_list)\n",
    "print(question_response_score_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
